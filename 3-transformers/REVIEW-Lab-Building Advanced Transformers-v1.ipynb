{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow  \n",
    "%pip install pyarrow\n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:03:07.249196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747699387.268484   13781 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747699387.274161   13781 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747699387.288570   13781 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747699387.288592   13781 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747699387.288594   13781 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747699387.288595   13781 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 21:03:07.293003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.993428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.773496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101.395427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.196135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.731793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99.781851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103.458576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101.885045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99.461251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>101.535345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close\n",
       "0  100.993428\n",
       "1   99.773496\n",
       "2  101.395427\n",
       "3  103.196135\n",
       "4   99.731793\n",
       "5   99.781851\n",
       "6  103.458576\n",
       "7  101.885045\n",
       "8   99.461251\n",
       "9  101.535345"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "    # For each word (query), which other words (keys) should i pay attention to, and how much?\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    # For each attention head, learn different patterns in different regions of the sequence\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    # Makes it possible for the model to focus on multiple positions and interactions in the sequence\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "\n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "\n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        # Attention output - normalized\n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "\n",
    "        # Feed foward output\n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "\n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement a full Transformer encoder \n",
    "\n",
    "Using the previous classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747701044.543553   13781 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7797 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dropout \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747701115.278644   18832 service.cc:152] XLA service 0x7c8e680029c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747701115.278661   18832 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-05-19 21:31:55.581241: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1747701117.105926   18832 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-05-19 21:32:00.457328: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 1412 bytes spill stores, 1356 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:00.613826: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:00.712371: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:01.595307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 652 bytes spill stores, 620 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:01.902509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.083226: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.091731: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.165761: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.309258: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1552 bytes spill stores, 1500 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.370258: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 424 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.389556: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 852 bytes spill stores, 852 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.720026: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:02.930611: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 724 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:03.090641: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:03.527591: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 676 bytes spill stores, 1068 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:03.631632: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 3224 bytes spill stores, 3232 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:03.795008: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:04.017381: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:05.386587: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 1872 bytes spill stores, 1864 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:05.568283: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:05.730972: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:06.155097: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:06.527246: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:06.624266: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 912 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.045296: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.189408: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.417726: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 516 bytes spill stores, 516 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.624902: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.639856: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.769151: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 1868 bytes spill stores, 1832 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:07.984235: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:08.040451: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:08.203566: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 916 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:08.565594: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 592 bytes spill stores, 592 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.089433: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.150385: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.225725: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.269409: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.363268: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.413514: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.422756: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:09.592286: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.173882: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.205715: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.224155: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.292421: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.294551: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.307650: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.830615: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:10.834155: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 1844 bytes spill stores, 1844 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/60\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.1961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747701140.784049   18832 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 13.6115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:32:24.875553: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:25.067915: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:26.047508: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1100 bytes spill stores, 1872 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:26.072767: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:26.858439: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:27.076857: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:27.208852: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 2916 bytes spill stores, 2888 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:27.291975: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1464 bytes spill stores, 1156 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:27.398653: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 172 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:28.716823: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 724 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:28.787417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:28.790623: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:29.642463: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:29.802656: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:30.032670: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:30.256664: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:30.578704: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:30.908886: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 916 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:31.042785: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:31.570570: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:31.661754: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:32.479918: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 516 bytes spill stores, 516 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:32.525733: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:32.698006: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:32.991821: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:33.039873: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:33.136374: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:33.611329: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:33.842570: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614_0', 380 bytes spill stores, 380 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:33.990656: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.044407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.597050: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.625824: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.745613: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.826710: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 912 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:34.968182: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.016364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.118042: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8561', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.306951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.324185: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.401532: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:32:35.759826: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8614', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 419ms/step - loss: 13.3111\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1753\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1680\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1976\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1331\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1739\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1629\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1053\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1553\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1210\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1242\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1011\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1130\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1033\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0671\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1225\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1067\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1154\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0496\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c8f70338970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 219ms/step - loss: 16.2362\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.6264\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.2646\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.4604\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.9238\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.4337\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.2246\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1181\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0653\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0556\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0411\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0400\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0266\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0270\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0350\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0239\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0218\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0194\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0195\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0213\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 6.6927e-04\n",
      "Test loss: 0.0010519272182136774\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = layers.Flatten()(encoder_outputs)\n",
    "dropout = layers.Dropout(0.5)(flatten)\n",
    "outputs = layers.Dense(1)(dropout)\n",
    "\n",
    "# Build the model\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=20,  batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:46:12.828386: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93_0', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:13.420089: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:14.087972: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 3224 bytes spill stores, 3232 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:14.145836: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87_0', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:14.154924: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1464 bytes spill stores, 1156 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:14.492321: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:14.606172: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:15.233094: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:15.598623: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:15.745192: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.020656: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.077375: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 172 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.496368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55_0', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.499843: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.525028: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:16.650320: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1100 bytes spill stores, 1872 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:17.161763: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25_0', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:18.018784: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:18.051265: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:18.276564: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:18.313225: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:19.073441: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:19.494228: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:19.592901: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.152210: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 916 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.235012: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 724 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.364797: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.415107: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.425908: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.814624: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 516 bytes spill stores, 516 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.927453: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:20.991411: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.233748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.393815: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.704504: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.722364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.817931: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:21.902240: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:22.027157: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:22.178876: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:22.268022: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:22.450617: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:22.808890: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 912 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:23.073331: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:23.405785: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:23.416672: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974_0', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "2025-05-19 21:46:23.552045: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57_0', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 113ms/step - loss: 0.3847\n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0380\n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0249\n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0245\n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0573\n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0481\n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0266\n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0235\n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0284\n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0155\n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0121\n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0142\n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0158\n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0176\n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0142\n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0118\n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0095\n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0072\n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0073\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0034\n",
      "Test loss for batch size 16: 0.002856798004359007\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=20,  batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss for batch size 16: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:47:20.453468: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:21.257487: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:21.359440: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 1412 bytes spill stores, 1356 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:21.378680: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:22.707389: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 3224 bytes spill stores, 3232 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:23.391561: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 676 bytes spill stores, 1068 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:23.753545: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 652 bytes spill stores, 620 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:23.864816: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:24.691130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27_0', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:24.794573: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 432 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:24.978562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 2620 bytes spill stores, 4476 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:25.338640: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 724 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:25.394231: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:25.438679: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 852 bytes spill stores, 852 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:26.431247: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:26.434539: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:26.949748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:26.971031: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.127300: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.159594: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.173757: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 516 bytes spill stores, 516 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.342607: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.411970: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.499732: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.536280: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 916 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.577582: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 424 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:27.638447: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:28.717052: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.041679: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921_0', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.414044: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.555994: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 912 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.556124: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.746102: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.908998: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:29.979658: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:30.303722: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:30.383744: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 40 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:30.685984: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:30.933255: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:30.946048: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.029216: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.276094: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.285537: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 1868 bytes spill stores, 1832 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.317869: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.337964: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.778379: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.785351: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:31.934139: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 592 bytes spill stores, 592 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:32.620836: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 1844 bytes spill stores, 1844 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:32.704120: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 1872 bytes spill stores, 1864 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:47:46.644516: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:46.669547: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:47.016713: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:47.488928: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:47.706760: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_93', 1320 bytes spill stores, 1144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:48.056960: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1464 bytes spill stores, 1156 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:48.382693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 2916 bytes spill stores, 2888 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:48.559359: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6_0', 1100 bytes spill stores, 1872 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:49.295856: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:49.635387: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:49.940817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:49.969730: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 648 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:50.295249: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:50.952726: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 172 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:51.549971: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 868 bytes spill stores, 868 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:51.600629: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:51.642737: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:51.645385: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 220 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:51.697179: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_87', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:52.127251: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 516 bytes spill stores, 516 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:52.399646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 724 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:52.410053: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:52.416148: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:52.620824: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:53.732867: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 108 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:53.749766: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 916 bytes spill stores, 916 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:53.948856: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921_0', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:54.274808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:54.282978: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:54.348518: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:54.537126: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.363353: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.475586: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 1868 bytes spill stores, 1832 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.790927: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.852622: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.933732: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:55.961808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:56.263254: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:56.265553: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:56.591933: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:56.802372: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.117365: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.118818: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 592 bytes spill stores, 592 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.174244: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.281982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 1844 bytes spill stores, 1844 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.403298: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.656142: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8921', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.692018: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 660 bytes spill stores, 660 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.809204: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.858797: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 1872 bytes spill stores, 1864 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:57.990916: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 912 bytes spill stores, 912 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:58.138791: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n",
      "2025-05-19 21:47:58.142445: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_8974', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 898ms/step - loss: 0.4226\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0229\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0055\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0035\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0029\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0030\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0026\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0022\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0023\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0021\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0023\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0023\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0027\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0023\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0021\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 7.1739e-04\n",
      "Test loss for batch size 64: 0.0007836775621399283\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=20,  batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss for batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 208ms/step - loss: 0.1161\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0127\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0101\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0109\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0068\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0041\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0042\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0026\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0024\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0016\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0022\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0021\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0016\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0016\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0025\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0024\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0014\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0015\n",
      "Test loss for tanh activation: 0.0010592490434646606\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "outputs = layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=20,  batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss for tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellisis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
